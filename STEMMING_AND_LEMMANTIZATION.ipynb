{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STEMMING AND LEMMANTIZATION.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw0iaRkRwACH"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov4TkF5hMjx0"
      },
      "source": [
        "##PORTER STEMMER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfut-zzp3fBu"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLrD5kch3m3n"
      },
      "source": [
        "p_stemmer = PorterStemmer()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eDtHJVK3rnp"
      },
      "source": [
        "words = ['have', 'having','had', 'hang', 'hanging', 'run', 'ran', 'runner', 'fair', 'fairly', 'nationality', 'nationalism', 'national', 'fairness']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_KrWLTg37I2",
        "outputId": "178c30bf-c542-4426-f5b0-6ec7cb90d37d"
      },
      "source": [
        "for word in words:\r\n",
        "  print(word + '----->' + p_stemmer.stem(word))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "have----->have\n",
            "having----->have\n",
            "had----->had\n",
            "hang----->hang\n",
            "hanging----->hang\n",
            "run----->run\n",
            "ran----->ran\n",
            "runner----->runner\n",
            "fair----->fair\n",
            "fairly----->fairli\n",
            "nationality----->nation\n",
            "nationalism----->nation\n",
            "national----->nation\n",
            "fairness----->fair\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SDaB6KiMojd"
      },
      "source": [
        "##SNOWBALL STEMMER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1syzEv0E4OaJ"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdkIRjlJ44LF"
      },
      "source": [
        "s_stemmer = SnowballStemmer(language = 'english')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQsJoa_O48IS",
        "outputId": "159b1631-9b01-4a70-f48a-c075726b4d00"
      },
      "source": [
        "for word in words:\r\n",
        "  print(word + '----->' + s_stemmer.stem(word))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "have----->have\n",
            "having----->have\n",
            "had----->had\n",
            "hang----->hang\n",
            "hanging----->hang\n",
            "run----->run\n",
            "ran----->ran\n",
            "runner----->runner\n",
            "fair----->fair\n",
            "fairly----->fair\n",
            "nationality----->nation\n",
            "nationalism----->nation\n",
            "national----->nation\n",
            "fairness----->fair\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1InS68B-Mzs0"
      },
      "source": [
        "##LEMMANTIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anzJ9Hg8C9nC"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsltZRdBEiml"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RiwHy02Enq7"
      },
      "source": [
        "doc = nlp(u'Peter Piper picked a peck of pickled peppers')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iFV2N8rE7OM",
        "outputId": "232716c4-6e49-4e75-8200-87babe66916f"
      },
      "source": [
        "for token in doc:\r\n",
        "  print(token, '\\t', token.dep_, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peter \t compound \t PROPN \t 12811822816318728671 \t Peter\n",
            "Piper \t nsubj \t PROPN \t 16272889730469212282 \t Piper\n",
            "picked \t ROOT \t VERB \t 14020768407998353649 \t pick\n",
            "a \t det \t DET \t 11901859001352538922 \t a\n",
            "peck \t dobj \t NOUN \t 17237996080690907035 \t peck\n",
            "of \t prep \t ADP \t 886050111519832510 \t of\n",
            "pickled \t amod \t VERB \t 15206147607577397413 \t pickle\n",
            "peppers \t pobj \t NOUN \t 7406717370878359568 \t pepper\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DffDaMUPoXy"
      },
      "source": [
        "##MATCHING AND VOCABULARY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13qmHEGZLEkk"
      },
      "source": [
        "##Rule-based Matching\r\n",
        "spaCy offers a rule-matching tool called Matcher that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zrWd6h7LoQa"
      },
      "source": [
        "from spacy.matcher import Matcher #IMPORT THE MATCHER LIBRARY\r\n",
        "matcher = Matcher(nlp.vocab)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtZSrc3MMJaz"
      },
      "source": [
        "#Creating patterns\r\n",
        "In literature, the phrase 'solar power' might appear as one word or two, with or without a hyphen. In this section we'll develop a matcher named 'SolarPower' that finds all three:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_n0H-1sL6fO"
      },
      "source": [
        "pattern1 = [{'ORTH': 'solarpower'}]\r\n",
        "pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\r\n",
        "pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\r\n",
        "\r\n",
        "matcher.add('SolarPower', None, pattern1, pattern2, pattern3)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkyZJNwOMVMx"
      },
      "source": [
        "#Applying the matcher to a Doc object\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_JtoD8hMQ-a"
      },
      "source": [
        "doc = nlp(u'The Solar Power industry continues to grow as demand \\\r\n",
        "for solarpower increases. Solar-power cars are gaining popularity.')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un8vvBrDMbjA",
        "outputId": "e7f14a7e-d9be-4abc-a160-fb1f3aacf669"
      },
      "source": [
        "found_matches = matcher(doc)  #matcher returns a list of tuples. Each tuple contains an ID for the match, with start & end tokens that map to the span doc[start:end]\r\n",
        "\r\n",
        "for match_id, start, end in found_matches:\r\n",
        "   print(found_matches)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n",
            "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n",
            "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO9_3tJDNRPA"
      },
      "source": [
        "##ON LEMMAS\r\n",
        "If we wanted to match on both 'solar power' and 'solar powered', it might be tempting to look for the lemma of 'powered' and expect it to be 'power'. This is not always the case! The lemma of the adjective 'powered' is still 'powered':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6dkYG09Meq2"
      },
      "source": [
        "pattern1 = [{'LOWER': 'solarpower'}]\r\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LEMMA': 'power'}] # CHANGE THIS PATTERN\r\n",
        "\r\n",
        "# Remove the old patterns to avoid duplication:\r\n",
        "matcher.remove('SolarPower')\r\n",
        "\r\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\r\n",
        "matcher.add('SolarPower', None, pattern1, pattern2)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ou_hVbANYkf"
      },
      "source": [
        "doc2 = nlp(u'Solar-powered energy runs solar-powered cars.')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzONbM2vNbYx",
        "outputId": "6e796e87-139b-4280-e997-fcb5fb7b1a3f"
      },
      "source": [
        "found_matches = matcher(doc2)\r\n",
        "print(found_matches)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBMsC_tmNu0g"
      },
      "source": [
        "##PhraseMatcher\r\n",
        "In the above section we used token patterns to perform rule-based matching. An alternative - and often more efficient - method is to match on terminology lists. In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into matcher instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbz32UB8NedR"
      },
      "source": [
        "from spacy.matcher import PhraseMatcher #IMPORTING PHRASE MATCHER LIBRARY\r\n",
        "matcher = PhraseMatcher(nlp.vocab)\r\n",
        "\r\n",
        "#For this exercise we're going to import a Wikipedia article on Reaganomics"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJldMgjvN6ih"
      },
      "source": [
        "with open('/content/Reaganomics - Wikipedia.html') as f:\r\n",
        "    doc3 = nlp(f.read())"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48IaDApWOCqA"
      },
      "source": [
        "# First, create a list of match phrases:\r\n",
        "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']\r\n",
        "\r\n",
        "# Next, convert each phrase to a Doc object:\r\n",
        "phrase_patterns = [nlp(text) for text in phrase_list]\r\n",
        "\r\n",
        "# Pass each Doc object into matcher (note the use of the asterisk!):\r\n",
        "matcher.add('VoodooEconomics', None, *phrase_patterns)\r\n",
        "\r\n",
        "# Build a list of matches:\r\n",
        "matches = matcher(doc3)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2LrkgoRO10r",
        "outputId": "75f4c11e-5776-4e48-8a83-0601eaa64ada"
      },
      "source": [
        "             # (match_id, start, end)\r\n",
        "             matches"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3473369816841043438, 12690, 12692)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuyUz545O78f",
        "outputId": "53fd264b-0836-4f3a-a735-3b32dbdc35f4"
      },
      "source": [
        "doc3[:70] #HERE WE ARE GETTING THE OUTPUT AS ELEMENTS OF AN HTML PAGE. THIS IS BEACUSE WE HAVE LOADED AN HTML DOC"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<!DOCTYPE html>\n",
              "<!-- saved from url=(0041)https://en.wikipedia.org/wiki/Reaganomics -->\n",
              "<html class=\"client-js ve-available\" lang=\"en\" dir=\"ltr\"><head><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n",
              "\n",
              "<title>Reaganomics - Wikipedia</title>\n",
              "<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":!1,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"YEr0P89Zdk4iy-DL7UMSPwAAAEk\",\"wgCSPNonce\":!1,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":!1,\"wgNamespaceNumber\":0,\"wgPageName\":\"Reaganomics\",\"wgTitle\":\"Reaganomics\",\"wgCurRevisionId\":1011394188,\"wgRevisionId\":1011394188,\"wgArticleId\":26529,\"wgIsArticle\":!0,\"wgIsRedirect\":!1,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"CS1 errors: missing periodical\",\"Webarchive template wayback links\",\"CS1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INhSUKD0PH13"
      },
      "source": [
        "##Viewing Matches\r\n",
        "here are a few ways to fetch the text surrounding a match. The simplest is to grab a slice of tokens from the doc that is wider than the match:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyaXeAPtPEQX",
        "outputId": "c3be6a86-79aa-4cf6-b36c-df6249b7d4c2"
      },
      "source": [
        "doc3[665:685]  # Note that the fifth match starts at doc3[673]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-color:transparent;padding:0}.mw-ui-button:active,.mw-ui-button.is-on{background-color:#c8ccd1;color:#000000;border-color:#72777d;box"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tpmlAKFPQAU",
        "outputId": "fc3d2fb9-e1fa-45cd-d6b1-7d7a4fb865fa"
      },
      "source": [
        "doc3[2975:2995]  # The sixth match starts at doc3[2985]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "mode:both;-moz-animation-fill-mode:both;-o-animation-fill-mode:both;animation-"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rub2G4TPUY7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}